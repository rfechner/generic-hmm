{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Optimization : A Connection to Deep Learning\n",
    "\n",
    "...Does this even work? seems like gradient methods fail or produce degenerate solutions for these type of problems...\n",
    "\n",
    "\n",
    "\n",
    "Up until now, we've avoided using the usual forward and backward pass notation to define our variables, as i wanted to draw a link between other timeseries methods. It turns out the *smoothened* hidden state distributions are the $\\gamma_t(\\cdot)$ variable used in HMM literature. But let's unroll the helper variables one by one and start with the *forward* variable $\\alpha$, which is a **unnormalized likelihood** $\\ell(X_t|Y_{0:t})$.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\alpha_t(j) &\\overset{\\text{Def.}}{=} (\\sum_{i=1}^N \\alpha_{t-1}(i)\\mathcal A[i, j]) \\mathcal B[j, Y_t]\n",
    "\\end{align*}\n",
    "\n",
    "We obtain the full data likelihood $P(Y|\\theta)$, by summing over $\\alpha_T(i)$. As all the computation is differentiable (only matrix multiplications), we can **back-propagate** through the computation graph and optimize via **gradient descent** using auto-diff and **Optimization constraints**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"numeric_optim.drawio.png\" alt=\"drawing\" width=\"500\"/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from scipy.optimize import minimize\n",
    "from functools import partial\n",
    "\n",
    "def likelihood(Bs : np.ndarray, y_i : np.ndarray, lengths : np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Bs : np.ndarray     of shape num_states * prod(num_obs_i).\n",
    "    offsets : List[int] contains num_obs - 1 indeces for the split.\n",
    "    returns: product of likelihoods for observation y_i\n",
    "    \n",
    "    \"\"\"\n",
    "    sections = np.insert(np.cumsum(lengths)[:-1], 0, 0)\n",
    "    indeces = y_i + sections\n",
    "    \n",
    "    arrs = Bs[:, indeces]\n",
    "\n",
    "    return np.prod(arrs, axis=1).squeeze()\n",
    "\n",
    "def neg_likelihood(pi, A, Bs, Y : NDArray[np.int64], lengths : NDArray[np.int64]) -> np.float32:\n",
    "    \"\"\"\n",
    "        Y : np.ndarray of shape (T, number of emission signals)\n",
    "    \"\"\"\n",
    "    alpha_tm1 : np.ndarray = pi * likelihood(Bs, Y[0, :], lengths)\n",
    "\n",
    "    for y_i in Y[1:]:\n",
    "        alpha_tm1 = A.T @ alpha_tm1 * likelihood(Bs, np.array(y_i), lengths)\n",
    "\n",
    "    return -np.sum(alpha_tm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 2\n",
    "lengths = np.array([2, 3])\n",
    "observations = np.array([[0, 1], [1, 0], [1, 1], [1, 0]]) # (D, not B), (C, B), (C, not B), (W, B)\n",
    "\n",
    "init_A = np.random.random(size=(num_states, num_states))\n",
    "init_A /= init_A.sum(axis=1)[:, None]\n",
    "\n",
    "Bs = [\n",
    "    np.random.random(size=(num_states, M)) for M in lengths\n",
    "]\n",
    "\n",
    "init_Bs = [\n",
    "    B / B.sum(axis=1)[:, None] for B in Bs\n",
    "]\n",
    "\n",
    "init_pi = np.ones(shape=(num_states,)) / num_states\n",
    "\n",
    "assert np.all([np.all([np.isclose(B.sum(axis=1), 1)]) for B in init_Bs]), 'Bs not stochastic'\n",
    "assert np.all(np.isclose(init_A.sum(axis=1), 1)), 'A not stochastic'\n",
    "assert np.isclose(init_pi.sum(), 1), 'pi not stochastic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn initial guesses into np arrays\n",
    "A = init_A.reshape(-1)\n",
    "Bs = np.concatenate(init_Bs, axis=1).reshape(-1)\n",
    "pi = np.array(init_pi).reshape(-1)\n",
    "\n",
    "params = np.concatenate([pi, A, Bs]).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0008095320162447279"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections = np.cumsum(lengths)[:-1]\n",
    "def func(params):\n",
    "    pi = softmax(params[:num_states])\n",
    "    A = softmax(params[num_states: num_states + num_states**2].reshape(num_states, num_states), axis=1)\n",
    "    Bs = params[num_states + num_states**2:].reshape(num_states, -1)\n",
    "    tmp = np.split(Bs, indices_or_sections=sections, axis=1)\n",
    "    Bs = np.concatenate([softmax(t, axis=1) for t in tmp], axis=1)\n",
    "    \n",
    "    return neg_likelihood(pi, A, Bs, Y=observations, lengths=lengths)\n",
    "func(params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = minimize(func, x0=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.00000000e+00, 1.57443794e-10]),\n",
       " array([[1.62162656e-07, 9.99999838e-01],\n",
       "        [1.48350553e-05, 9.99985165e-01]]),\n",
       " array([[9.99998410e-01, 1.58970096e-06, 2.47026841e-07, 9.99999753e-01,\n",
       "         6.47434816e-13],\n",
       "        [2.55023442e-14, 1.00000000e+00, 6.66661418e-01, 3.33338582e-01,\n",
       "         1.34945182e-11]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = softmax(results.x[:num_states])\n",
    "A = softmax(results.x[num_states: num_states + num_states**2].reshape(num_states, num_states), axis=1)\n",
    "Bs = results.x[num_states + num_states**2:].reshape(num_states, -1)\n",
    "tmp = np.split(Bs, indices_or_sections=sections, axis=1)\n",
    "Bs = np.concatenate([softmax(t, axis=1) for t in tmp], axis=1)\n",
    "\n",
    "pi, A, Bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[9.99998410e-01, 1.58970096e-06],\n",
       "        [2.55023442e-14, 1.00000000e+00]]),\n",
       " array([[2.47026841e-07, 9.99999753e-01, 6.47434816e-13],\n",
       "        [6.66661418e-01, 3.33338582e-01, 1.34945182e-11]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.split(Bs, indices_or_sections=sections, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generic-hmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
