{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Model Study\n",
    "\n",
    "Goal: Revisit Hidden Markov Models from the start, write slow but readable code. Build a Hidden Markov Model from scratch.\n",
    "\n",
    "Stretch goals: \n",
    "- revisit Theoretic foundations for Multivariate Categorically distributed emissions.\n",
    "- how does the independence assumptions for multivariate emissions change the theory?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the Defense of Simple Models\n",
    "\n",
    "Motivating Hidden Markov Models is hard without asking some fundamental questions about statistical and probabilistic inference and learning.\n",
    "\n",
    "Why do we model things? What are we interested about finding out - the underlying dynamics or solely predictions into the future? Do we even want to predict into the future or are we satisfied with estimating some underlying state responsible for our observations?\n",
    "Why not choose Timeseries model type X over type Y? Well, there is no easy answer to this, it may just be that another type of model is better fit for a specific requirement.\n",
    "\n",
    "Markov Models and *Hidden* Markov Models are so-called **generative** models. This means that after building them through a process called *training* or *fitting*, we may produce new pseudo-data from our model. We may extrapolate from timepoint $t$ onwards or simulate a new trajectory. We may choose different initial conditions and see, how the model evolves. It is here, i want to make an argument for Hidden Markov Models in the age of Neural Networks. \n",
    "The biggest advantage of HMMs over Neural Methods is in my understanding, that they are **interpretable**. They are white-box models, allowing the observer to clearly understand **why** the model has produced any output. To some degree, the data **is** the model. All important information has been distilled and transformed into a model.\n",
    "\n",
    "Another big advantage is that they are **simple** models. No-hyperparamter-tuning. No tinkering with the learning rate or swapping out optimizers, adding dropout, adding layer norms, removing layer norms... The process of fitting the model is more sane. In some sense this is a half-truth, as we do have to make assumptions when we choose a model type, however they are very different types of assumptions. They encourage us to think harder about the problem and the setup, not decide on model changes depending on whether or not the $F_1$-Score is moving up or down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions.\n",
    "\n",
    "Let a Hidden Markov Model be a tuple $\\lambda = (\\mathcal A, \\mathcal B, \\pi)$ where \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal A := p(x_{t}|X_{0:t-1})\\quad & \\text{Conditional distribution over hidden state given previous hidden states}\\\\\n",
    "    \\mathcal B := p(y_{t}|X_{0:t})\\quad& \\text{Conditional distribution over emission signal given current and previous hidden states}\\\\ \n",
    "    \\pi := p(x_0)\\quad & \\text{Initial state distribution}\n",
    "\\end{align*}\n",
    "\n",
    "In my thesis and the following study, we will look at a special case of HMMs, where $\\mathcal A, \\mathcal B$ and $\\pi$ are categorical distributions, making them row-stochastic matrices containing transition and emission probabilities. The probabilistic formulation of the object lets us see however, how we might use **Bayes-Rule** to do inference on the hidden states, as\n",
    "\n",
    "$$\n",
    "    p(X_{0:t}|y_t) = \\frac{p(y_t|X_{0:t})p(X_{0:t})}{p(y_t)} \\quad \\text{ or } \\quad p(X_{0:N}| Y_{0:N}) = \\frac{p(Y_{0:N}|X_{0:N})p(X_{0:N})}{p(Y_{0:N})}\n",
    "$$\n",
    "\n",
    "Calculating these expressions however is untractable, as it would require us to compute large integrals $p(Y) = \\int p(Y|X)p(X) dX$ where $p(X) = p(x_0)p(x_1|x_0)\\dots p(x_N|x_0, \\dots, x_{N-1})$ itself would be tricky to compute. It is here where we make **simplifying independence assumptions** called the *Markov Assumption*:\n",
    "\n",
    "$$\n",
    "    p(x_t|x_0, x_1, \\dots x_{t-1}) = p(x_t|x_{t-1}) \\quad \\text{and} \\quad p(y_t|x_0, x_1, \\dots, x_{t}) = p(y_t|x_t)\n",
    "$$\n",
    "\n",
    "In words, we assume that transitions only depend on the previously visited state and that emissions only depend on the current hidden state. This simplifies the computations, as we may factor expressions as $p(X)$ into parts. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductory Example\n",
    "\n",
    "We are concerned with making statements about any **hidden** or **latent** variable $x$. In this example, lets consider the following setup: You're sitting at home, listening to the weather news on the radio. The true weather however is not-observable, as your window blinds are down. In our model, the true weather will be the non-observable (i.e. the *hidden* variable $x_t$) and the news will be the observable variable $y_t$. There are only two different 'true' weather states: Sunny $S$ and Rainy $R$, whereas the media reports three different observations: Dry $D$, Wet $W$ and Cloudy $C$.\n",
    "The prior over the hidden states is a discrete uniform distribution. Additionally, we **know** $\\mathcal A, \\mathcal B$. \n",
    "\n",
    "**IMPORTANT** This is obviously a shortcut. In reality we dont know $\\mathcal A$ and $\\mathcal B$, we have to fit them, using our data! Notice, how our motivation has changed the model and how we use it. Instead of being interested *how exactly* the weather changes or in which hidden state what wheather observation is made, we focus on prediction rather than trying to understand underlying dynamics.\n",
    "\n",
    "We summarize the assumptions with these equations:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal A &= \\begin{bmatrix}\n",
    "        P(S|S) & P(R|S)\\\\\n",
    "        P(S|R) & P(R|R)\n",
    "    \\end{bmatrix} = \\begin{bmatrix}\n",
    "        0.7 & 0.3\\\\\n",
    "        0.2 & 0.8\n",
    "    \\end{bmatrix}\\\\\n",
    "    \\mathcal B &= \\begin{bmatrix}\n",
    "        P(D|S) & P(W|S) & P(C|S)\\\\\n",
    "        P(D|R) & P(W|R) & P(C|R)\n",
    "    \\end{bmatrix} = \\begin{bmatrix}\n",
    "        0.5 & 0.3 & 0.2\\\\\n",
    "        0.0 & 0.8 & 0.2\n",
    "    \\end{bmatrix}\\\\\n",
    "    \\pi &= \\begin{bmatrix}\n",
    "        P(x_0 = S)\\\\\n",
    "        P(x_0 = R)\n",
    "    \\end{bmatrix} = \\begin{bmatrix}\n",
    "        0.5\\\\\n",
    "        0.5\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting and Updating\n",
    "\n",
    "Inference on timeseries is usually separated into three steps: **predicting** the next hidden state $\\tilde x = p(x_t| X_{0:t-1}, Y_{0:t-1})$ and **updating** said prediction once we have observed the current emission $\\hat x = p(x_t | X_{0:t-1}, Y_{0:t-1}, y_t)$\n",
    "\n",
    "The recursive prediction equation is:\n",
    "\n",
    "$$\n",
    "    p(x_t| Y_{0:t-1}) = \\int \\underbrace{p(x_t|x_{t-1})}_{\\text{hidden state dynamics}}\\cdot \\overbrace{p(x_{t-1}|Y_{0:t-1})}^{\\text{previous update result} } dx_{t-1}  \n",
    "$$\n",
    "\n",
    "This is a local integral over $x_{t-1}$, which we may be able to solve, if we have access to the previous update result, which is similarily recursively defined as:\n",
    "\n",
    "$$\n",
    "    p(x_t|Y_{0:t}) = \\frac{\\overbrace{p(y_t|x_t)}^{\\text{likelihood}}\\overbrace{p(x_t|Y_{0:t-1})}^{\\quad \\text{prev. prediction result}}}{\\int p(y_t | x_t )p(x_t | Y_{0:tâˆ’1}) dx_t}\n",
    "$$\n",
    "\n",
    "We can see, that we have to compute the prediction and the update recursively, in order to get an updated estimate of the distribution over $x_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation\n",
    "\n",
    "Now, how would we go about calculating these quantities? We should start at the beginning, calculating the prediction of $x_1$, before updating it with our observation. For the sake of this example, lets assume we've observed the sequence $Y_{0:3} = \\{ D, C, C, W\\}$ and the state and emission matrices are as described above.\n",
    "\n",
    "Calculate the update for $t=0$:\n",
    "\n",
    "\\begin{align*}\n",
    "    p(x_0) &= \\pi\\\\\n",
    "    p(x_0|y_0) &= \\frac{p(y_0|x_0)p(x_0)}{\\int p(y_0|x_0)p(x_0) dx_0} = \\frac{\\begin{bmatrix}\n",
    "        P(D|S)\\\\\n",
    "        P(D|R)\n",
    "    \\end{bmatrix}\\odot \\begin{bmatrix}\n",
    "        P(S)\\\\\n",
    "        P(R)\n",
    "    \\end{bmatrix}}{P(D|S)P(S) + P(D|R)P(R)}\\\\\n",
    "    &= \\frac{\\begin{bmatrix}\n",
    "        0.5\\\\\n",
    "        0.0\n",
    "    \\end{bmatrix}\\odot \\begin{bmatrix}\n",
    "        0.5\\\\\n",
    "        0.5\n",
    "    \\end{bmatrix}}{0.5 \\cdot 0.5 + 0.0 \\cdot 0.5} = \\begin{bmatrix}\n",
    "        1\\\\\n",
    "        0\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Seems like after seeing the first observation $y_0 = D$, we are completely convinced, that it is sunny outside. Lets see, what happens, if we predict the next hidden state.\n",
    "\n",
    "\\begin{align*}\n",
    "    p(x_0|y_0) &= \\begin{bmatrix}\n",
    "        1\\\\\n",
    "        0\n",
    "    \\end{bmatrix}\\\\\n",
    "    p(x_1|x_0, y_0) &= \\int p(x_1|x_0)p(x_0|y_0)dx_0\\\\\n",
    "    &= \\mathcal A ^T p(x_0|y_0)\\\\\n",
    "    &= \\begin{bmatrix}\n",
    "        0.7\\\\\n",
    "        0.3\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets update for our estimate of the hidden state at $t=1$. For this, we calculate the update analogously to our update at timestep $t= 0$, with the difference, that now we use the posterior of the prediction as the prior.\n",
    "\n",
    "\\begin{align*}\n",
    "    p(x_1) &= p(x_1|y_0, x_0) = \\begin{bmatrix}\n",
    "    0.7\\\\\n",
    "    0.3\n",
    "    \\end{bmatrix}\\\\\n",
    "    p(x_1|Y_{0:1}) &= \\frac{\\mathcal B[:, 2] \\odot p(x_1|y_0, x_0)}{\\sum_i \\mathcal B[:, 2] \\odot p(x_1|y_0, x_0)}\\\\\n",
    "    &= \\frac{\\begin{bmatrix}\n",
    "        0.2\\\\\n",
    "        0.2\n",
    "    \\end{bmatrix}\\odot \\begin{bmatrix}\n",
    "        0.7\\\\\n",
    "        0.3\n",
    "    \\end{bmatrix}}{0.2 \\cdot 0.7 + 0.2 \\cdot 0.3}\\\\\n",
    "    &= \\begin{bmatrix}\n",
    "        0.7\\\\\n",
    "        0.3\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\\\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will put this prediction and update routine in code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([\n",
    "    [0.7, 0.3],\n",
    "    [0.2, 0.8]\n",
    "])\n",
    "\n",
    "B = np.array([\n",
    "    [0.5, 0.2, 0.3],\n",
    "    [0.0, 0.2, 0.8]\n",
    "])\n",
    "\n",
    "x_0 = np.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(x_tm1 : np.ndarray, y_t : int,  B : np.ndarray) -> np.ndarray:\n",
    "\n",
    "    likelihood = B[:, y_t]\n",
    "    prior = x_tm1\n",
    "\n",
    "    posterior_unnormalized = likelihood * prior\n",
    "    return posterior_unnormalized / posterior_unnormalized.sum()\n",
    "\n",
    "def predict(x_tm1 : np.ndarray, A : np.ndarray) -> np.ndarray:\n",
    "\n",
    "    # check for row-stochasticity\n",
    "    assert np.all(np.isclose(A.sum(axis=1), 1)), \"Matrix A doesn't seem to be row-stochastic\"\n",
    "\n",
    "    #check if x_tm1 is a valid probability distribution\n",
    "    assert np.isclose(x_tm1.sum(), 1), \"x_tm1 isn't stochastic, sum != 1\"\n",
    "\n",
    "    return A.T @ np.atleast_1d(x_tm1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([0.5, 0.5]),\n",
       "  array([0.7, 0.3]),\n",
       "  array([0.55, 0.45]),\n",
       "  array([0.475, 0.525]),\n",
       "  array([0.32666667, 0.67333333])],\n",
       " [array([1., 0.]),\n",
       "  array([0.7, 0.3]),\n",
       "  array([0.55, 0.45]),\n",
       "  array([0.25333333, 0.74666667])])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations = [0, 1, 1, 2] # [D, C, C, W]\n",
    "x_tm1 = x_0\n",
    "predictions = [x_0]\n",
    "updates = []\n",
    "\n",
    "for y_i in observations:\n",
    "    x_updated = update(x_tm1, y_i, B)\n",
    "    x_tm1 = predict(x_updated, A)\n",
    "\n",
    "    updates.append(x_updated)\n",
    "    predictions.append(x_tm1)\n",
    "\n",
    "    \n",
    "predictions, updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Case\n",
    "\n",
    "Before looking at smoothing, let us think about changing the likelihood function $p(y_t | x_t)$. What would change, if y_t would be a collection of $K$ independent categorically distributed random variables instead of a single random variable? We would have to keep track of $K$ different emission matrices and calculate the update differently, as $p(y_t|x_t)$ now expands into $\\prod_{i = 1}^K p(y^i_t | x_t)$.\n",
    "\n",
    "We extend our example of the TV and weather with another observation variable. Lets assume we would observe dogs barking depending on the weather. If it's sunny, people much rather go out to walk their dog than if its raining. From this assumption follows the emission matrix $\\mathcal B_2 = \\begin{bmatrix}\n",
    "    P(B|S) & P(\\neg B|S)\\\\\n",
    "    P(B|R) & P(\\neg B|R)\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    0.6 & 0.4\\\\\n",
    "    0.1 & 0.9\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [0.7, 0.3],\n",
    "    [0.2, 0.8]\n",
    "])\n",
    "\n",
    "B_1 = np.array([\n",
    "    [0.5, 0.2, 0.3],\n",
    "    [0.0, 0.2, 0.8]\n",
    "])\n",
    "\n",
    "B_2 = np.array([\n",
    "    [0.6, 0.4],\n",
    "    [0.1, 0.9]\n",
    "])\n",
    "\n",
    "x_0 = np.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mv(x_tm1 : np.ndarray, y_t : Sequence[int],  Bs : Sequence[np.ndarray]) -> np.ndarray:\n",
    "\n",
    "    \"\"\"\n",
    "        x_tm1   : np.ndarray        vector of hidden state distribution from timestep t-1\n",
    "        y_t     : Sequence[int]     Sequence of ints, int at index i contains observation for corresponding emission matrix at index i\n",
    "        Bs      : Sequence[np.ndarray]  Sequence of emission matrices              \n",
    "    \"\"\"\n",
    "\n",
    "    # likelihood p(y_t | x_t) factorizes into p(y^1_t|x_t) * p(y^2_t|x_t) * ... *  p(y^K_t|x_t)\n",
    "    likelihoods = np.ones_like(x_tm1)\n",
    "    for y, B in zip(y_t, Bs):\n",
    "        likelihoods *= B[:, y]\n",
    "    prior = x_tm1\n",
    "    \n",
    "    posterior_unnormalized = likelihoods * prior\n",
    "\n",
    "    # possible underflow -> we'll fix it later on\n",
    "    return posterior_unnormalized / posterior_unnormalized.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([0.5, 0.5]),\n",
       "  array([0.7, 0.3]),\n",
       "  array([0.66666667, 0.33333333]),\n",
       "  array([0.43529412, 0.56470588]),\n",
       "  array([0.61111111, 0.38888889])],\n",
       " [array([1., 0.]),\n",
       "  array([0.93333333, 0.06666667]),\n",
       "  array([0.47058824, 0.52941176]),\n",
       "  array([0.82222222, 0.17777778])])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations = [[0, 1], [1, 0], [1, 1], [1, 0]] # (D, not B), (C, B), (C, not B), (W, B)\n",
    "B = [B_1, B_2]\n",
    "x_tm1 = x_0\n",
    "predictions = [x_0]\n",
    "updates = []\n",
    "\n",
    "for y_i in observations:\n",
    "    x_updated = update_mv(x_tm1, y_i, B)\n",
    "    x_tm1 = predict(x_updated, A)\n",
    "\n",
    "    updates.append(x_updated)\n",
    "    predictions.append(x_tm1)\n",
    "\n",
    "    \n",
    "predictions, updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "\n",
    "Say, we've observed up until timestep $T$ and want to incorporate this additional information into our estimates of previous hidden states.\n",
    "Mathematically, we'd be interested in finding $p(x_t|Y_{0:T})$ or in words: \"given all the information we've observed, what is the distribution over the hidden states at timestep $t$?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can derive the form of the smoothed distribution as\n",
    "\n",
    "$$\n",
    "    p(x_t|Y) = \\overbrace{p(x_t|Y_{0:t})}^{\\text{filtered result}} \\int p(x_{t+1} | x_t) \\frac{\\overbrace{p(x_{t+1}|Y)}^{\\text{prev. smoothed result}}}{\\underbrace{p(x_{t+1}|Y_{0:t})}_{\\text{prediction result}}}dx_{t+1}\n",
    "$$\n",
    "\n",
    "\n",
    "(For details, please see Phillip Hennigs Lecture \"Probabilistic Machine Learning\" Lecture 20: Gauss Markov Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation\n",
    "\n",
    "Lets assume the same setup as in the univariate case before and calculate the smoothened distribution for timestep $t=2$, assuming $p(x_3|Y) = p(x_3|Y_{0:3})$:\n",
    "\n",
    "\\begin{align*}\n",
    "    p(x_2|Y) &= \\overbrace{p(x_2|Y_{0:2})}^{\\text{filtered result}} \\int p(x_{3} | x_2) \\frac{\\overbrace{p(x_{3}|Y)}^{\\text{prev. smoothed result}}}{\\underbrace{p(x_{3}|Y_{0:2})}_{\\text{prediction result}}}dx_{3}\\\\\n",
    "    &= \\begin{bmatrix}\n",
    "        P(x_2 = S | Y_{0:2})\\\\\n",
    "        P(x_2 = R | Y_{0:2})\n",
    "    \\end{bmatrix} \\odot \\mathcal A \\frac{p(x_3|Y)}{p(x_3|Y_{0:2})}\\\\\n",
    "    &= \\begin{bmatrix}\n",
    "        P(x_2 = S | Y_{0:2})\\\\\n",
    "        P(x_2 = R | Y_{0:2})\n",
    "    \\end{bmatrix} \\odot \\mathcal A \\begin{bmatrix}\n",
    "        \\frac{P(x_3 = S | Y)}{P(x_3 = S | Y_{0:2})}\\\\\n",
    "        \\frac{P(x_3 = R | Y)}{P(x_3 = R | Y_{0:2})}\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\odot$ denotes pointwise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [0.7, 0.3],\n",
    "    [0.2, 0.8]\n",
    "])\n",
    "\n",
    "B = np.array([\n",
    "    [0.5, 0.2, 0.3],\n",
    "    [0.0, 0.2, 0.8]\n",
    "])\n",
    "\n",
    "x_0 = np.array([0.5, 0.5])\n",
    "observations = [0, 1, 1, 2] # [D, C, C, W]\n",
    "x_tm1 = x_0\n",
    "predictions = [x_0]\n",
    "updates = []\n",
    "\n",
    "\n",
    "for y_i in observations:\n",
    "    x_updated = update(x_tm1, y_i, B)\n",
    "    x_tm1 = predict(x_updated, A)\n",
    "\n",
    "    updates.append(x_updated)\n",
    "    predictions.append(x_tm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(t : int, x_tp1 : np.ndarray, A : np.ndarray) -> np.ndarray:\n",
    "    \n",
    "    filtered = updates[t]\n",
    "    predicted = predictions[t + 1]\n",
    "    u = A @ (x_tp1 / predicted)\n",
    "    ret = filtered * u\n",
    "\n",
    "    #print(f\"Filtered: {filtered}, Predicted : {predicted}, A@(x_tp1/pred) : {u}, Ret: {ret}\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed = [updates[-1]]\n",
    "x_tp1 = updates[-1]\n",
    "\n",
    "for t in range(len(observations) - 2, -1, -1):\n",
    "    # print(f\"t: {t}, x_tp1: {x_tp1}\")\n",
    "    x_tp1 = smooth(t, x_tp1, A)\n",
    "    smoothed.append(x_tp1)\n",
    "\n",
    "smoothed = list(reversed(smoothed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated belief: [1. 0.], Smoothed belief: [1. 0.]\n",
      "Updated belief: [0.7 0.3], Smoothed belief: [0.65333333 0.34666667]\n",
      "Updated belief: [0.55 0.45], Smoothed belief: [0.44 0.56]\n",
      "Updated belief: [0.25333333 0.74666667], Smoothed belief: [0.25333333 0.74666667]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(observations)):\n",
    "    print(f'Updated belief: {updates[i]}, Smoothed belief: {smoothed[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Case:\n",
    "\n",
    "The current implementation of ```smooth``` would actually work already, we only have to switch back to our multivariate implementation of the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [0.7, 0.3],\n",
    "    [0.2, 0.8]\n",
    "])\n",
    "\n",
    "B_1 = np.array([\n",
    "    [0.5, 0.2, 0.3],\n",
    "    [0.0, 0.2, 0.8]\n",
    "])\n",
    "\n",
    "B_2 = np.array([\n",
    "    [0.6, 0.4],\n",
    "    [0.1, 0.9]\n",
    "])\n",
    "\n",
    "x_0 = np.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [[0, 1], [1, 0], [1, 1], [1, 0]] # (D, not B), (C, B), (C, not B), (W, B)\n",
    "B = [B_1, B_2]\n",
    "x_tm1 = x_0\n",
    "predictions = [x_0]\n",
    "updates = []\n",
    "\n",
    "for y_i in observations:\n",
    "    x_updated = update_mv(x_tm1, y_i, B)\n",
    "    x_tm1 = predict(x_updated, A)\n",
    "\n",
    "    updates.append(x_updated)\n",
    "    predictions.append(x_tm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated belief: [1. 0.], Smoothed belief: [1. 0.]\n",
      "Updated belief: [0.93333333 0.06666667], Smoothed belief: [0.93333333 0.06666667]\n",
      "Updated belief: [0.47058824 0.52941176], Smoothed belief: [0.66666667 0.33333333]\n",
      "Updated belief: [0.82222222 0.17777778], Smoothed belief: [0.82222222 0.17777778]\n"
     ]
    }
   ],
   "source": [
    "smoothed = [updates[-1]]\n",
    "x_tp1 = updates[-1]\n",
    "\n",
    "for t in range(len(observations) - 2, -1, -1):\n",
    "    # print(f\"t: {t}, x_tp1: {x_tp1}\")\n",
    "    x_tp1 = smooth(t, x_tp1, A)\n",
    "    smoothed.append(x_tp1)\n",
    "\n",
    "smoothed = list(reversed(smoothed))\n",
    "for i in range(len(observations)):\n",
    "    print(f'Updated belief: {updates[i]}, Smoothed belief: {smoothed[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most likely Hidden-State-Sequence\n",
    "\n",
    "The **Viterbi**-Algorithm is used to find the most likely state-sequence to have produced the observations. It works by making use of dynamic programming, keeping track of all possible best state sequences, then backtracking to get the best sequence. Here is some latex extracted from my thesis, explaining the steps of the algorithm:\n",
    "\n",
    "Notation: $b_j(o_t)$ indexes into position $(j, o_t)$ of the emission matrix $\\mathcal B$, where $o_t$ is the index of the observation at timestep $t$.\n",
    "\n",
    "Similarily, $a_{ij}$ indexes into position $(i,j)$ of the transition matrix $\\mathcal A$.\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    & \\text{Initialization}:\\\\\n",
    "    \n",
    "\n",
    "    &\\begin{align*}\n",
    "        & \\delta_1(i) = \\pi_ib_i(o_1), \\hspace{1cm} 1 \\leq i \\leq N\\\\\n",
    "        & \\psi_1(i) = 0\n",
    "    \\end{align*}{}\\\\\n",
    "    \n",
    "    & \\text{Recursion}:\\\\\n",
    "    \n",
    "\n",
    "    &\\begin{align*}\n",
    "        & \\delta_t(j) = \\max_{1 \\leq i \\leq N}(\\delta_{t-1}(i)a_{ij})b_j(o_t) &1 \\leq j \\leq N\\\\\n",
    "        &&2 \\leq t \\leq T\\\\\n",
    "        & \\psi_t(j) = \\argmax_{1 \\leq i \\leq N}(\\delta_{t-1}(i)a_{ij}) &1 \\leq j \\leq N\\\\\n",
    "        &&2 \\leq t \\leq T\n",
    "    \\end{align*}{}\\\\\n",
    "    \n",
    "    & \\text{Termination}:\\\\\n",
    "    \n",
    "\n",
    "    &\\begin{align*}\n",
    "        & P^* = \\max_{1 \\leq i \\leq N}(\\delta_T(i))\\\\\n",
    "        & q_T^* = \\argmax_{1 \\leq i \\leq N}(\\delta_T(i))\n",
    "    \\end{align*}{}\\\\\n",
    "    \n",
    "    & \\text{Backtracking}:\\\\\n",
    "    \n",
    "\n",
    "    &q_t^* = \\psi_{t+1}(q_{t+1}^*), \\hspace{0.2cm} t = T-1,T-2,...,1\n",
    "    \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Below, i'll provide a python implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(observations : Sequence[int], \n",
    "            A : np.ndarray, \n",
    "            B : np.ndarray, \n",
    "            pi : np.ndarray) -> Sequence[int]:\n",
    "\n",
    "    num_observations : int = len(observations)\n",
    "    num_states : int = len(pi)\n",
    "\n",
    "    # shape: (num_states, num_observations)\n",
    "    delta : np.ndarray = np.zeros(shape=(num_states, num_observations))\n",
    "    psi : np.ndarray = np.zeros_like(delta)\n",
    "\n",
    "\n",
    "    # initialization\n",
    "    delta[:, 0] = pi * B[:, observations[0]]\n",
    "\n",
    "    # recursion/iteration\n",
    "    for t in range(1, num_observations):\n",
    "        for j in range(num_states):\n",
    "            transitions = delta[:, t-1] * A[:, j]\n",
    "            max_i = np.argmax(transitions)\n",
    "            psi[j,t] = max_i\n",
    "            delta[j,t] = np.max(transitions) *\\\n",
    "                  B[j, observations[t]]\n",
    "\n",
    "    print(delta)\n",
    "    print(psi)\n",
    "    # termination\n",
    "    q_T : int = np.argmax(delta[:, -1], keepdims=False)\n",
    "    state_sequence_reversed : Sequence[int] = [q_T]\n",
    "\n",
    "    q_prime : int = q_T\n",
    "    for t in range(num_observations - 2, -1, -1):\n",
    "        q_prime = int(psi[q_prime, t + 1])\n",
    "        state_sequence_reversed.append(q_prime)\n",
    "\n",
    "    return list(reversed(state_sequence_reversed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25     0.035    0.0049   0.001029]\n",
      " [0.       0.015    0.0024   0.001536]]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 1. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lets test our viterbi implementation\n",
    "\n",
    "A = np.array([\n",
    "    [0.7, 0.3],\n",
    "    [0.2, 0.8]\n",
    "])\n",
    "\n",
    "B = np.array([\n",
    "    [0.5, 0.2, 0.3],\n",
    "    [0.0, 0.2, 0.8]\n",
    "])\n",
    "\n",
    "pi = np.array(\n",
    "    [0.5, 0.5]\n",
    ")\n",
    "observations = [0, 1, 1, 2] # [D, C, C, W]\n",
    "\n",
    "MLE_state_sequence : Sequence[int] = viterbi(observations=observations,\n",
    "                                             A=A,\n",
    "                                             B=B,\n",
    "                                             pi=pi)\n",
    "MLE_state_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check whether the conditional data likelihood is maximized for the found sequence:\n",
    "\n",
    "def cond_data_likelihood(states : Sequence[int], observations : Sequence[int], A, B, pi) -> float:\n",
    "\n",
    "    likelihood = pi[states[0]] * B[states[0], observations[0]]\n",
    "    for t in range(1, len(states)):\n",
    "        likelihood *= A[states[t-1], states[t]] * B[states[t], observations[t]]\n",
    "    \n",
    "    return likelihood\n",
    "\n",
    "possible_states = itertools.product([0,1], repeat=len(observations))\n",
    "likelihoods = [(p, cond_data_likelihood(p, observations, A, B, pi)) for p in possible_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 1, 1, 1), 0.0015360000000000005),\n",
       " ((0, 0, 1, 1), 0.0013440000000000001),\n",
       " ((0, 0, 0, 1), 0.0011759999999999997),\n",
       " ((0, 0, 0, 0), 0.0010289999999999997),\n",
       " ((0, 1, 0, 1), 0.000144),\n",
       " ((0, 1, 1, 0), 0.000144),\n",
       " ((0, 0, 1, 0), 0.000126),\n",
       " ((0, 1, 0, 0), 0.000126),\n",
       " ((1, 0, 0, 0), 0.0),\n",
       " ((1, 0, 0, 1), 0.0),\n",
       " ((1, 0, 1, 0), 0.0),\n",
       " ((1, 0, 1, 1), 0.0),\n",
       " ((1, 1, 0, 0), 0.0),\n",
       " ((1, 1, 0, 1), 0.0),\n",
       " ((1, 1, 1, 0), 0.0),\n",
       " ((1, 1, 1, 1), 0.0)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_likelihoods = list(sorted(likelihoods, key=lambda t : t[1], reverse=True))\n",
    "sorted_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we found the right solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Viterbi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_mv(observations : Sequence[Sequence[int]], \n",
    "            A : np.ndarray, \n",
    "            Bs : Sequence[np.ndarray], \n",
    "            pi : np.ndarray) -> Sequence[int]:\n",
    "    \"\"\"\n",
    "    Viterbi algorithm for multivariate discrete emissions.\n",
    "\n",
    "    NOTE: observations are of shape (N, D) where D is the number of emission signals.\n",
    "    The ordering of these signals MUST correspond to the ordering in the list of emission matrices.\n",
    "\n",
    "    \"\"\"\n",
    "    num_observations : int = len(observations)\n",
    "    num_states : int = len(pi)\n",
    "\n",
    "    # shape: (num_states, num_observations)\n",
    "    delta : np.ndarray = np.zeros(shape=(num_states, num_observations))\n",
    "    psi : np.ndarray = np.zeros_like(delta)\n",
    "\n",
    "\n",
    "    # initialization\n",
    "    delta[:, 0] = pi * np.prod([B[:, observations[0]] for B in Bs])\n",
    "\n",
    "    # recursion/iteration\n",
    "    for t in range(1, num_observations):\n",
    "        for j in range(num_states):\n",
    "            transitions = delta[:, t-1] * A[:, j]\n",
    "            max_i = np.argmax(transitions)\n",
    "            psi[j,t] = max_i\n",
    "            delta[j,t] = np.max(transitions) * \\\n",
    "                np.prod([B[j, observations[t][i]] for i, B in enumerate(Bs)]) \n",
    "\n",
    "    # termination\n",
    "    q_T : int = np.argmax(delta[:, -1], keepdims=False)\n",
    "    state_sequence_reversed : Sequence[int] = [q_T]\n",
    "\n",
    "    q_prime : int = q_T\n",
    "    for t in range(num_observations - 2, -1, -1):\n",
    "        q_prime = int(psi[q_prime, t + 1])\n",
    "        state_sequence_reversed.append(q_prime)\n",
    "\n",
    "    return list(reversed(state_sequence_reversed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [0.7, 0.3],\n",
    "    [0.2, 0.8]\n",
    "])\n",
    "\n",
    "B_1 = np.array([\n",
    "    [0.5, 0.2, 0.3],\n",
    "    [0.0, 0.2, 0.8]\n",
    "])\n",
    "\n",
    "B_2 = np.array([\n",
    "    [0.6, 0.4],\n",
    "    [0.1, 0.9]\n",
    "])\n",
    "\n",
    "pi = np.array([0.5, 0.5])\n",
    "observations = [[0, 1], [1, 0], [1, 1], [1, 0]] # (D, not B), (C, B), (C, not B), (W, B)\n",
    "Bs = [B_1, B_2]\n",
    "\n",
    "MLE_state_sequence : Sequence[int] = viterbi_mv(observations=observations,\n",
    "                                             A=A,\n",
    "                                             Bs=Bs,\n",
    "                                             pi=pi)\n",
    "MLE_state_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check mv case\n",
    "\n",
    "def cond_data_likelihood_mv(states : Sequence[int], \n",
    "                            observations : Sequence[Sequence[int]], \n",
    "                            A : np.ndarray, \n",
    "                            Bs : Sequence[np.ndarray], \n",
    "                            pi : np.ndarray) -> float:\n",
    "\n",
    "    likelihood = pi[states[0]] * np.prod([B[states[0], observations[0][i]] for i, B in enumerate(Bs)])\n",
    "    \n",
    "    for t in range(1, len(states)):\n",
    "        likelihood *= A[states[t-1], states[t]] * np.prod([B[states[t], observations[t][i]] for i, B in enumerate(Bs)])\n",
    "    \n",
    "    return likelihood\n",
    "\n",
    "possible_states = itertools.product([0,1], repeat=len(observations))\n",
    "likelihoods = [(p, cond_data_likelihood_mv(p, observations, A, Bs, pi)) for p in possible_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 0, 0, 0), 3.95136e-05),\n",
       " ((0, 0, 1, 0), 1.0886400000000001e-05),\n",
       " ((0, 0, 1, 1), 7.257600000000002e-06),\n",
       " ((0, 0, 0, 1), 2.8224000000000007e-06),\n",
       " ((0, 1, 1, 0), 2.073600000000001e-06),\n",
       " ((0, 1, 1, 1), 1.382400000000001e-06),\n",
       " ((0, 1, 0, 0), 8.064000000000003e-07),\n",
       " ((0, 1, 0, 1), 5.760000000000004e-08),\n",
       " ((1, 0, 0, 0), 0.0),\n",
       " ((1, 0, 0, 1), 0.0),\n",
       " ((1, 0, 1, 0), 0.0),\n",
       " ((1, 0, 1, 1), 0.0),\n",
       " ((1, 1, 0, 0), 0.0),\n",
       " ((1, 1, 0, 1), 0.0),\n",
       " ((1, 1, 1, 0), 0.0),\n",
       " ((1, 1, 1, 1), 0.0)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_likelihoods = list(sorted(likelihoods, key=lambda t : t[1], reverse=True))\n",
    "sorted_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Numerics\n",
    "\n",
    "The way we've been calculating the probabilities and likelihoods is fine for any small number of states and observations, but lets see what happens, if we want to calculate the conditional data likelihood for a sequence, which is a bit longer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations = [0]*1000 # observing only dry weather\n",
    "states = [0]*1000 # only sunshine\n",
    "cond_data_likelihood(states = states, observations=observations, A=A, B=B, pi=pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... the probability is just so tiny, that python decides it is not worth the precision...\n",
    "\n",
    "For now we will simply ignore that this is a problem and move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generic-hmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
